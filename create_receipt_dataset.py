#!/usr/bin/env python3
"""
é¦™æ¸¯æ”¶æ“š OCR æ•¸æ“šé›†å‰µå»ºå·¥å…·
ä½¿ç”¨ EasyOCR è‡ªå‹•ç”Ÿæˆæ¨™è¨»
ç”Ÿæˆç¬¦åˆ deep-text-recognition-benchmark çš„ gt.txt æ ¼å¼(åˆ‡å‰²æ–‡å­—å€åŸŸç”¨æ–¼ Recognition è¨“ç·´)
æ”¯æ´å½æ›²æ”¶æ“šçš„è‡ªå‹•æ ¡æ­£åŠŸèƒ½
"""

import json
import cv2
import easyocr
import numpy as np
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple
import argparse


class ReceiptDatasetCreator:
    """æ”¶æ“šæ•¸æ“šé›†å‰µå»ºå™¨"""

    # é¡åˆ¥å¸¸æ•¸ - é…ç½®åƒæ•¸
    CONFIDENCE_THRESHOLD = 0.5
    MAX_DEVIATION_THRESHOLD = 50
    TILT_ANGLE_THRESHOLD = 2
    MIN_LINES_FOR_CURVE_DETECTION = 5
    CLAHE_CLIP_LIMIT = 2.0
    CLAHE_GRID_SIZE = (8, 8)
    DENOISE_H = 7
    SHARPEN_STRENGTH = 0.5

    # æ¨¡å‹å¿«å– (å–®ä¾‹æ¨¡å¼)
    _reader_cache = {}
    _doctr_cache = None

    @classmethod
    def get_reader(cls, langs=('ch_tra', 'en'), gpu=True):
        """ç²å–å¿«å–çš„ EasyOCR Reader (å–®ä¾‹æ¨¡å¼)"""
        cache_key = (tuple(langs), gpu)
        if cache_key not in cls._reader_cache:
            print(f"ğŸ”„ Loading EasyOCR model ({', '.join(langs)})...")
            cls._reader_cache[cache_key] = easyocr.Reader(list(langs), gpu=gpu)
            print("âœ… EasyOCR model loaded!")
        return cls._reader_cache[cache_key]

    @classmethod
    def get_doctr_model(cls):
        """ç²å–å¿«å–çš„ doctr æ¨¡å‹ (å–®ä¾‹æ¨¡å¼)"""
        if cls._doctr_cache is None and DOCTR_AVAILABLE:
            try:
                print("ğŸ”„ Loading doctr model for document correction...")
                cls._doctr_cache = ocr_predictor(pretrained=True)
                print("âœ… doctr model loaded!")
            except Exception as e:
                print(f"âš ï¸  Failed to load doctr: {e}")
        return cls._doctr_cache

    def __init__(self, input_dir: str = "./input", processed_dir: str = "./processed",
                 crops_dir: str = "./processed/crops", dataset_dir: str = "./dataset_gt",
                 enable_correction: bool = False):
        # è¼¸å…¥é©—è­‰
        if not input_dir or not isinstance(input_dir, str):
            raise ValueError(f"Invalid input_dir: {input_dir}")

        self.input_dir = Path(input_dir).resolve()
        self.processed_dir = Path(processed_dir).resolve()
        self.crops_dir = Path(crops_dir).resolve()
        self.dataset_dir = Path(dataset_dir).resolve()
        self.enable_correction = False  # å¼·åˆ¶åœç”¨åœ–åƒè™•ç†

        # å‰µå»ºç›®éŒ„çµæ§‹
        self.annotations_file = self.processed_dir / "annotations.json"
        self.processed_images_dir = self.processed_dir / "images"
        self.train_dir = self.dataset_dir / "train"
        self.valid_dir = self.dataset_dir / "valid"
        self.test_dir = self.dataset_dir / "test"

        for dir_path in [self.input_dir, self.processed_dir, self.processed_images_dir,
                         self.crops_dir, self.train_dir, self.valid_dir, self.test_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # å»¶é²è¼‰å…¥ EasyOCR æ¨¡å‹ (åªåœ¨éœ€è¦ OCR æ™‚æ‰è¼‰å…¥)
        self.reader = None

        # æ¨™è¨»æ•¸æ“š
        self.annotations = {}
        self.load_annotations()

    def load_annotations(self):
        """è¼‰å…¥å·²æœ‰çš„æ¨™è¨»"""
        if self.annotations_file.exists():
            try:
                with open(self.annotations_file, 'r', encoding='utf-8') as f:
                    self.annotations = json.load(f)
                print(f"ğŸ“‚ Loaded {len(self.annotations)} existing annotations")
            except (json.JSONDecodeError, IOError) as e:
                print(f"âš ï¸  Failed to load annotations: {e}")
                self.annotations = {}
            except Exception as e:
                print(
                    f"âŒ Unexpected error loading annotations: {type(e).__name__}: {e}")
                self.annotations = {}

    def save_annotations(self):
        """ä¿å­˜æ¨™è¨»"""
        try:
            with open(self.annotations_file, 'w', encoding='utf-8') as f:
                json.dump(self.annotations, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Saved annotations to {self.annotations_file}")
        except (IOError, OSError) as e:
            print(f"âŒ Failed to save annotations: {e}")
        except Exception as e:
            print(
                f"âŒ Unexpected error saving annotations: {type(e).__name__}: {e}")

    def correct_document_distortion(self, image: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        æº«å’Œçš„æ–‡æª”æ ¡æ­£ - åªä¿®æ­£æ˜é¡¯çš„é€è¦–å‚¾æ–œ,ä¿ç•™åœ–ç‰‡ç´°ç¯€
        Returns: (æ ¡æ­£å¾Œçš„åœ–åƒ, æ˜¯å¦é€²è¡Œäº†æ ¡æ­£)
        """
        # è¼¸å…¥é©—è­‰
        if image is None or not isinstance(image, np.ndarray):
            raise ValueError("Invalid image: expected numpy array")
        if image.size == 0:
            raise ValueError("Invalid image: empty array")

        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(
                image.shape) == 3 else image

            # 1. æª¢æ¸¬é‚Šç·£
            edges = cv2.Canny(gray, 50, 150, apertureSize=3)

            # 2. ä½¿ç”¨ Hough è®Šæ›æª¢æ¸¬ç›´ç·š
            lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100,
                                    minLineLength=100, maxLineGap=10)

            if lines is None or len(lines) < 10:
                return image, False

            # 3. è¨ˆç®—ä¸»è¦è§’åº¦(æª¢æ¸¬æ˜¯å¦å‚¾æ–œ)
            angles = []
            for line in lines:
                x1, y1, x2, y2 = line[0]
                angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi
                angles.append(angle)

            # æ‰¾åˆ°ä¸»è¦è§’åº¦
            median_angle = np.median(angles)

            # å¦‚æœå‚¾æ–œè§’åº¦å¾ˆå°(<2åº¦),ä¸éœ€è¦æ ¡æ­£
            if abs(median_angle) < self.TILT_ANGLE_THRESHOLD:
                return image, False

            # å¦‚æœå‚¾æ–œæ˜é¡¯,é€²è¡Œæ—‹è½‰æ ¡æ­£
            if abs(median_angle) > self.TILT_ANGLE_THRESHOLD and abs(median_angle) < 45:
                h, w = image.shape[:2]
                center = (w // 2, h // 2)

                # è¨ˆç®—æ—‹è½‰çŸ©é™£
                rotation_matrix = cv2.getRotationMatrix2D(
                    center, float(median_angle), 1.0)

                # è¨ˆç®—æ–°çš„é‚Šç•Œå¤§å°
                cos_val = abs(rotation_matrix[0, 0])
                sin_val = abs(rotation_matrix[0, 1])
                new_w = int((h * sin_val) + (w * cos_val))
                new_h = int((h * cos_val) + (w * sin_val))

                # èª¿æ•´æ—‹è½‰çŸ©é™£
                rotation_matrix[0, 2] += (new_w / 2) - center[0]
                rotation_matrix[1, 2] += (new_h / 2) - center[1]

                # æ‡‰ç”¨æ—‹è½‰
                corrected = cv2.warpAffine(image, rotation_matrix, (new_w, new_h),
                                           flags=cv2.INTER_LINEAR,
                                           borderMode=cv2.BORDER_CONSTANT,
                                           borderValue=(255, 255, 255))

                print(f"   âœ… æ ¡æ­£å‚¾æ–œ {median_angle:.1f}Â°")
                return corrected, True

            return image, False

        except cv2.error as e:
            print(f"   âš ï¸  OpenCV éŒ¯èª¤: {e}")
            return image, False
        except (ValueError, TypeError) as e:
            print(f"   âš ï¸  æ•¸æ“šéŒ¯èª¤: {e}")
            return image, False
        except Exception as e:
            print(f"   âŒ  æœªé æœŸçš„éŒ¯èª¤: {type(e).__name__}: {e}")
            return image, False

    def dewarp_curved_document(self, img: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        æª¢æ¸¬ä¸¦æ ¡æ­£å½æ›²çš„æ–‡æª” (æº«å’Œè™•ç†,ä¿ç•™ç´°ç¯€)

        è¿”å›:
            (æ ¡æ­£å¾Œçš„åœ–ç‰‡, æ˜¯å¦æª¢æ¸¬åˆ°å½æ›²)
        """
        # è¼¸å…¥é©—è­‰
        if img is None or not isinstance(img, np.ndarray):
            raise ValueError("Invalid image: expected numpy array")
        if img.size == 0:
            raise ValueError("Invalid image: empty array")

        try:
            # 1. å…ˆå¢å¼·å°æ¯”åº¦
            enhanced = self.enhance_image_quality(img)

            # 2. è½‰ç‚ºç°åº¦åœ–é€²è¡Œé‚Šç·£æª¢æ¸¬
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“æ˜¯ç°åº¦åœ–
            if len(enhanced.shape) == 2:
                gray = enhanced
            else:
                gray = cv2.cvtColor(enhanced, cv2.COLOR_BGR2GRAY)

            # 3. æª¢æ¸¬æ°´å¹³ç·šæ¢
            edges = cv2.Canny(gray, 50, 150)
            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100,
                                    minLineLength=100, maxLineGap=10)

            if lines is None or len(lines) < self.MIN_LINES_FOR_CURVE_DETECTION:
                return img, False

            # 4. æª¢æŸ¥ç·šæ¢æ˜¯å¦æœ‰æ˜é¡¯å½æ›²
            deviations = []
            for line in lines:
                x1, y1, x2, y2 = line[0]
                # è¨ˆç®—ç·šæ¢ä¸­é»åˆ°ç†æƒ³ç›´ç·šçš„åå·®
                mid_x = (x1 + x2) / 2
                mid_y = (y1 + y2) / 2
                ideal_y = y1 + (y2 - y1) * (mid_x - x1) / \
                    (x2 - x1) if x2 != x1 else y1
                deviation = abs(mid_y - ideal_y)
                deviations.append(deviation)

            max_deviation = max(deviations) if deviations else 0

            # 5. å¦‚æœåå·®è¶…éé–¾å€¼,æ¨™è¨˜ç‚ºå½æ›²(ä½†ä¸åšæ¿€é€²æ ¡æ­£)
            is_curved = max_deviation > self.MAX_DEVIATION_THRESHOLD

            if is_curved:
                print(f"   âš ï¸  æª¢æ¸¬åˆ°å½æ›² (æœ€å¤§åå·®: {max_deviation:.1f}px)")
                print(f"   ğŸ’¡ å»ºè­°: ä½¿ç”¨å¹³æ•´çš„è¡¨é¢é‡æ–°æ‹æ”,æˆ–æ‰‹å‹•ä½¿ç”¨ Photoshop/GIMP æ ¡æ­£")

            # è¿”å›åŸåœ– - æˆ‘å€‘ä¸åšæ¿€é€²çš„å»å½æ›²,åªæé†’ç”¨æˆ¶
            return img, is_curved

        except cv2.error as e:
            print(f"   âš ï¸  OpenCV éŒ¯èª¤: {e}")
            return img, False
        except (ValueError, TypeError) as e:
            print(f"   âš ï¸  æ•¸æ“šéŒ¯èª¤: {e}")
            return img, False
        except Exception as e:
            print(f"   âŒ  æœªé æœŸçš„éŒ¯èª¤: {type(e).__name__}: {e}")
            return img, False

    def enhance_image_quality(self, image: np.ndarray) -> np.ndarray:
        """
        æº«å’Œåœ°å¢å¼·åœ–åƒè³ªé‡ - ä¿ç•™ç´°ç¯€,é©åˆ OCR

        Args:
            image: è¼¸å…¥åœ–ç‰‡ (BGR å½©è‰²)

        Returns:
            å¢å¼·å¾Œçš„åœ–ç‰‡ (ç°åº¦åœ–)
        """
        # è¼¸å…¥é©—è­‰
        if image is None or not isinstance(image, np.ndarray):
            raise ValueError("Invalid image: expected numpy array")
        if image.size == 0:
            raise ValueError("Invalid image: empty array")

        try:
            # 1. è½‰ç°éš
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image.copy()

            # 2. è¼•å¾®å»å™ª (ä¿ç•™æ–‡å­—é‚Šç·£)
            denoised = cv2.fastNlMeansDenoising(gray, h=self.DENOISE_H)

            # 3. è‡ªé©æ‡‰ç›´æ–¹åœ–å‡è¡¡åŒ– (CLAHE) - å¢å¼·å°æ¯”åº¦
            clahe = cv2.createCLAHE(clipLimit=self.CLAHE_CLIP_LIMIT,
                                    tileGridSize=self.CLAHE_GRID_SIZE)
            enhanced = clahe.apply(denoised)

            # 4. è¼•å¾®éŠ³åŒ– (å¢å¼·æ–‡å­—é‚Šç·£)
            kernel = np.array([[-1, -1, -1],
                              [-1, 9, -1],
                              [-1, -1, -1]])
            sharpened = cv2.filter2D(
                enhanced, -1, kernel * self.SHARPEN_STRENGTH)
            sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)

            return sharpened

        except cv2.error as e:
            print(f"   âš ï¸  OpenCV éŒ¯èª¤: {e}")
            return image if len(image.shape) == 2 else cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        except (ValueError, TypeError) as e:
            print(f"   âš ï¸  æ•¸æ“šéŒ¯èª¤: {e}")
            return image if len(image.shape) == 2 else cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        except Exception as e:
            print(f"   âŒ  æœªé æœŸçš„éŒ¯èª¤: {type(e).__name__}: {e}")
            return image if len(image.shape) == 2 else cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    def preprocess_image(self, image_path: Path) -> np.ndarray:
        """
        ç°¡å–®è®€å–åœ–ç‰‡ - ä¸åšä»»ä½•è™•ç†

        Returns:
            åŸå§‹åœ–ç‰‡
        """
        # è¼¸å…¥é©—è­‰
        if not isinstance(image_path, Path):
            image_path = Path(image_path)
        if not image_path.exists():
            raise FileNotFoundError(f"Image file not found: {image_path}")

        img = cv2.imread(str(image_path))
        if img is None:
            raise ValueError(f"Cannot read image: {image_path}")

        return img

    def ocr_image(self, image_path: Path) -> Dict:
        """ä½¿ç”¨ EasyOCR è­˜åˆ¥åœ–ç‰‡ä¸¦åˆ‡å‰²æ–‡å­—å€åŸŸ"""
        print(f"\nğŸ” Processing: {image_path.name}")

        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥ (å»¶é²è¼‰å…¥)
        if self.reader is None:
            self.reader = self.get_reader()

        # è®€å–åŸåœ–
        img = self.preprocess_image(image_path)

        # ç›´æ¥ä½¿ç”¨åŸåœ–é€²è¡Œ OCR
        result = self.reader.readtext(img)

        # æ•´ç†çµæœ - åªä¿ç•™é«˜ä¿¡å¿ƒåº¦çš„çµæœï¼Œä¸¦åˆ‡å‰²æ–‡å­—å€åŸŸ
        ocr_results = []
        full_text_lines = []
        filtered_count = 0
        base_name = Path(image_path).stem

        for idx, (bbox, text, confidence) in enumerate(result):
            # éæ¿¾ä½ä¿¡å¿ƒåº¦çµæœ
            if confidence < self.CONFIDENCE_THRESHOLD:
                filtered_count += 1
                continue

            # å°‡ numpy æ•¸çµ„è½‰æ›ç‚º Python list
            bbox_list = [[float(x), float(y)] for x, y in bbox]

            # åˆ‡å‰²æ–‡å­—å€åŸŸä¸¦ä¿å­˜åˆ° crops/ ç›®éŒ„
            try:
                cropped_img = self.crop_text_regions(image_path, bbox_list)
                if cropped_img is not None and cropped_img.size > 0:
                    crop_filename = f"{base_name}_crop_{idx:03d}.jpg"
                    crop_path = self.crops_dir / crop_filename
                    cv2.imwrite(str(crop_path), cropped_img)

                    ocr_results.append({
                        'bbox': bbox_list,
                        'text': text,
                        'confidence': float(confidence),
                        'crop_filename': crop_filename
                    })
                    full_text_lines.append(text)
            except Exception as e:
                print(f"   âš ï¸  åˆ‡å‰²å€åŸŸ {idx} å¤±æ•—: {e}")
                continue

        if filtered_count > 0:
            print(
                f"   ğŸ” éæ¿¾æ‰ {filtered_count} å€‹ä½ä¿¡å¿ƒåº¦çµæœ (< {self.CONFIDENCE_THRESHOLD})")

        # ä¿å­˜åŸåœ–åˆ° processed/images ç›®éŒ„
        processed_img_path = self.processed_images_dir / image_path.name
        shutil.copy(str(image_path), str(processed_img_path))

        return {
            'image_name': image_path.name,
            'original_image_path': str(image_path),
            'processed_image_path': str(processed_img_path),
            'ocr_results': ocr_results,
            'full_text': '\n'.join(full_text_lines),
            'timestamp': datetime.now().isoformat(),
            'verified': False  # æ¨™è¨˜æ˜¯å¦å·²äººå·¥é©—è­‰
        }

    def auto_generate_annotations(self, overwrite: bool = False):
        """è‡ªå‹•ç‚º input/ ç›®éŒ„ä¸­çš„æ‰€æœ‰åœ–ç‰‡ç”Ÿæˆæ¨™è¨»"""
        import gc  # åƒåœ¾å›æ”¶

        image_files = list(self.input_dir.glob('*.jpg')) + \
            list(self.input_dir.glob('*.jpeg')) + \
            list(self.input_dir.glob('*.png'))

        print(f"\nğŸ“¸ Found {len(image_files)} images in {self.input_dir}")

        for idx, img_path in enumerate(image_files, 1):
            print(f"\n[{idx}/{len(image_files)}] Processing {img_path.name}")

            # è·³éå·²è™•ç†çš„åœ–ç‰‡
            if img_path.name in self.annotations and not overwrite:
                print(f"â­ï¸  Skipping (already processed)")
                continue

            try:
                annotation = self.ocr_image(img_path)
                self.annotations[img_path.name] = annotation

                # é¡¯ç¤ºè­˜åˆ¥çµæœ
                print(
                    f"âœ… Detected {len(annotation['ocr_results'])} text regions")
                print(f"ğŸ“ Preview:")
                print("-" * 60)
                print(annotation['full_text'][:500])  # é¡¯ç¤ºå‰ 500 å­—ç¬¦
                if len(annotation['full_text']) > 500:
                    print("...")
                print("-" * 60)

            except Exception as e:
                print(f"âŒ Error processing {img_path.name}: {e}")

            finally:
                # è¨˜æ†¶é«”ç®¡ç†: æ¯è™•ç† 10 å¼µåœ–ç‰‡å¾ŒåŸ·è¡Œåƒåœ¾å›æ”¶
                if idx % 10 == 0:
                    gc.collect()
                    print(
                        f"   ğŸ§¹ Memory cleanup (processed {idx}/{len(image_files)})")

        # ä¿å­˜æ¨™è¨»
        self.save_annotations()
        print(
            f"\nâœ… Auto-annotation complete! Processed {len(image_files)} images")

        # æœ€çµ‚è¨˜æ†¶é«”æ¸…ç†
        gc.collect()

    def crop_text_regions(self, image_path: Path, bbox, padding: int = 5):
        """
        æ ¹æ“š bbox åˆ‡å‰²æ–‡å­—å€åŸŸ

        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            bbox: æ–‡å­—æ¡†åæ¨™ [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
            padding: é‚Šç•Œå¡«å……åƒç´ 

        Returns:
            åˆ‡å‰²å¾Œçš„åœ–ç‰‡ (numpy array)
        """
        # è¼¸å…¥é©—è­‰
        if not isinstance(image_path, Path):
            image_path = Path(image_path)
        if not image_path.exists():
            raise FileNotFoundError(f"Image file not found: {image_path}")

        if not isinstance(bbox, (list, np.ndarray)) or len(bbox) != 4:
            raise ValueError(
                f"Invalid bbox format: expected 4 points, got {len(bbox) if bbox else 'None'}")

        if padding < 0:
            raise ValueError(f"Padding must be non-negative, got {padding}")

        img = cv2.imread(str(image_path))
        if img is None:
            raise FileNotFoundError(f"Cannot read image: {image_path}")

        try:
            # ç²å– bbox çš„æœ€å°å¤–æ¥çŸ©å½¢
            points = np.array(bbox, dtype=np.float32)
            x_min = max(0, int(np.min(points[:, 0])) - padding)
            y_min = max(0, int(np.min(points[:, 1])) - padding)
            x_max = min(img.shape[1], int(np.max(points[:, 0])) + padding)
            y_max = min(img.shape[0], int(np.max(points[:, 1])) + padding)

            # é©—è­‰è£åˆ‡å€åŸŸ
            if x_max <= x_min or y_max <= y_min:
                raise ValueError(
                    f"Invalid crop region: x({x_min},{x_max}), y({y_min},{y_max})")

            # åˆ‡å‰²åœ–ç‰‡
            cropped = img[y_min:y_max, x_min:x_max]

            return cropped

        except (ValueError, TypeError) as e:
            print(f"   âš ï¸  è£åˆ‡å¤±æ•—: {e}")
            return None
        except Exception as e:
            print(f"   âŒ  æœªé æœŸçš„éŒ¯èª¤: {type(e).__name__}: {e}")
            return None

    def generate_training_dataset(self, train_ratio: float = 0.8, valid_ratio: float = 0.1,
                                  crop_text_regions: bool = True):
        """
        ç”Ÿæˆè¨“ç·´æ•¸æ“šé›† - gt.txt æ ¼å¼ (ç”¨æ–¼ deep-text-recognition-benchmark)

        Args:
            train_ratio: è¨“ç·´é›†æ¯”ä¾‹
            valid_ratio: é©—è­‰é›†æ¯”ä¾‹
            crop_text_regions: æ˜¯å¦åˆ‡å‰²æ–‡å­—å€åŸŸ (True=è¨“ç·´Recognition, False=è¨“ç·´å®Œæ•´OCR)

        ç›®éŒ„çµæ§‹ (crop_text_regions=True):
        dataset_gt/
        â”œâ”€â”€ train/
        â”‚   â”œâ”€â”€ receipt001_crop_000.jpg  # ç¬¬ä¸€å€‹æ–‡å­—å€åŸŸ
        â”‚   â”œâ”€â”€ receipt001_crop_001.jpg  # ç¬¬äºŒå€‹æ–‡å­—å€åŸŸ
        â”‚   â””â”€â”€ gt.txt

        gt.txt æ ¼å¼ (tab åˆ†éš”):
        receipt001_crop_000.jpg	SUPERNORMAL
        receipt001_crop_001.jpg	ç¸½è¨ˆ
        """
        # è¼¸å…¥é©—è­‰
        if not (0 < train_ratio < 1):
            raise ValueError(
                f"Invalid train_ratio: {train_ratio}, must be between 0 and 1")
        if not (0 <= valid_ratio < 1):
            raise ValueError(
                f"Invalid valid_ratio: {valid_ratio}, must be between 0 and 1")
        if train_ratio + valid_ratio > 1:
            raise ValueError(
                f"train_ratio + valid_ratio must be <= 1, got {train_ratio + valid_ratio}")

        # æª¢æŸ¥æ˜¯å¦æœ‰å·²é©—è­‰çš„ OCR çµæœ (æª¢æŸ¥ ocr_results å±¤ç´šè€Œä¸æ˜¯åœ–ç‰‡å±¤ç´š)
        verified_count = 0
        annotations_with_verified = {}

        for image_name, anno in self.annotations.items():
            verified_regions = [
                ocr for ocr in anno.get('ocr_results', [])
                if ocr.get('verified', False)
            ]
            if verified_regions:
                annotations_with_verified[image_name] = anno
                verified_count += len(verified_regions)

        if verified_count == 0:
            print(
                "âŒ No verified annotations found! Please run manual verification first.")
            print("ğŸ’¡ Or run with --auto-verify to skip manual verification")
            return

        print(
            f"âœ… Found {verified_count} verified text regions across {len(annotations_with_verified)} images")

        print(
            f"\nğŸ“Š Generating training dataset from {verified_count} verified text regions")
        print(
            f"ğŸ“ Mode: {'Crop text regions (Recognition training)' if crop_text_regions else 'Full image (Complete OCR training)'}")
        print(f"ğŸ“„ Format: gt.txt (tab-separated, for deep-text-recognition-benchmark)")

        # ä½¿ç”¨æœ‰é©—è­‰çµæœçš„æ¨™è¨»
        verified = annotations_with_verified

        if crop_text_regions:
            # æ¨¡å¼ 1: æŒ‰ crop (æ–‡å­—å€åŸŸ) åˆ†å‰²æ•¸æ“šé›†
            # æ”¶é›†æ‰€æœ‰å·²é©—è­‰çš„ crop
            all_crops = []
            for image_name, anno in verified.items():
                for ocr_result in anno.get('ocr_results', []):
                    if ocr_result.get('verified', False):
                        all_crops.append({
                            'image_name': image_name,
                            'anno': anno,
                            'ocr_result': ocr_result
                        })

            # æ‰“äº‚ä¸¦åˆ†å‰²
            np.random.seed(42)
            np.random.shuffle(all_crops)

            n_total = len(all_crops)
            n_train = int(n_total * train_ratio)
            n_valid = int(n_total * valid_ratio)

            train_crops = all_crops[:n_train]
            valid_crops = all_crops[n_train:n_train + n_valid]
            test_crops = all_crops[n_train + n_valid:]

            print(
                f"ğŸ“ˆ Split by crops: Train={len(train_crops)}, Valid={len(valid_crops)}, Test={len(test_crops)}")

            # è½‰æ›ç‚ºèˆŠæ ¼å¼ (ç‚ºäº†ç›¸å®¹å¾ŒçºŒä»£ç¢¼)
            def crops_to_items(crops):
                # å°‡ crop åˆ—è¡¨è½‰æ›ç‚º {image_name: anno} æ ¼å¼ï¼Œä½†æ¨™è¨˜å“ªäº› ocr_result å±¬æ–¼é€™å€‹ split
                items_dict = {}
                for crop in crops:
                    img_name = crop['image_name']
                    if img_name not in items_dict:
                        items_dict[img_name] = {
                            'anno': crop['anno'],
                            'crop_indices': []
                        }
                    # æ‰¾åˆ°é€™å€‹ ocr_result åœ¨åŸå§‹åˆ—è¡¨ä¸­çš„ç´¢å¼•
                    ocr_idx = crop['anno']['ocr_results'].index(
                        crop['ocr_result'])
                    items_dict[img_name]['crop_indices'].append(ocr_idx)

                return [(k, v['anno'], v['crop_indices']) for k, v in items_dict.items()]

            train_items = crops_to_items(train_crops)
            valid_items = crops_to_items(valid_crops)
            test_items = crops_to_items(test_crops)

        else:
            # æ¨¡å¼ 2: æŒ‰åœ–ç‰‡åˆ†å‰²æ•¸æ“šé›† (å®Œæ•´ OCR è¨“ç·´)
            items = list(verified.items())
            np.random.seed(42)
            np.random.shuffle(items)

            n_total = len(items)
            n_train = int(n_total * train_ratio)
            n_valid = int(n_total * valid_ratio)

            # ç¢ºä¿ valid è‡³å°‘æœ‰ 1 å¼µåœ–ç‰‡ (å¦‚æœç¸½æ•¸ >= 3)
            if n_total >= 3 and n_valid == 0:
                n_valid = 1
                n_train = n_total - n_valid - \
                    max(1, int(n_total * (1 - train_ratio - valid_ratio)))

            train_items = [(k, v, None) for k, v in items[:n_train]]
            valid_items = [(k, v, None)
                           for k, v in items[n_train:n_train + n_valid]]
            test_items = [(k, v, None) for k, v in items[n_train + n_valid:]]

            print(
                f"ğŸ“ˆ Split by images: Train={len(train_items)}, Valid={len(valid_items)}, Test={len(test_items)}")

        # ç”Ÿæˆæ•¸æ“šé›†æ–‡ä»¶ (gt.txt æ ¼å¼)
        for split_name, split_items in [
            ('train', train_items),
            ('valid', valid_items),
            ('test', test_items)
        ]:
            if not split_items:
                continue

            split_dir = self.dataset_dir / split_name
            split_dir.mkdir(exist_ok=True)

            # å‰µå»º gt.txt (tab åˆ†éš”æ ¼å¼)
            gt_file = split_dir / 'gt.txt'

            print(f"\nğŸ“ Creating {split_name} dataset...")

            total_samples = 0

            try:
                with open(gt_file, 'w', encoding='utf-8') as f:

                    for item in split_items:
                        # è§£åŒ…æ–°æ ¼å¼: (image_name, anno, crop_indices)
                        if len(item) == 3:
                            image_name, anno, crop_indices = item
                        else:
                            # å‘å¾Œå…¼å®¹èˆŠæ ¼å¼
                            image_name, anno = item
                            crop_indices = None

                        src_img = Path(anno['processed_image_path'])

                        if not src_img.exists():
                            print(f"  âš ï¸  Image not found: {src_img}")
                            continue

                        if crop_text_regions:
                            # æ¨¡å¼ 1: ä½¿ç”¨å·²åˆ‡å‰²çš„æ–‡å­—å€åŸŸ (å¾ crops/ ç›®éŒ„)
                            # å¦‚æœæœ‰ crop_indicesï¼Œåªè™•ç†é€™äº›ç´¢å¼•çš„ OCR çµæœ
                            ocr_results = anno['ocr_results']

                            if crop_indices is not None:
                                # åªè™•ç†æŒ‡å®šçš„ crop
                                ocr_results_to_process = [
                                    ocr_results[i] for i in crop_indices if i < len(ocr_results)]
                            else:
                                # è™•ç†æ‰€æœ‰å·²é©—è­‰çš„ crop
                                ocr_results_to_process = [
                                    ocr for ocr in ocr_results if ocr.get('verified', False)]

                            for ocr_result in ocr_results_to_process:
                                text = ocr_result['text'].strip()
                                crop_filename = ocr_result.get('crop_filename')
                                confidence = ocr_result['confidence']

                                if not text or not crop_filename:
                                    continue

                                # æ¸…ç†æ–‡å­— (ç§»é™¤æ›è¡Œç¬¦å’Œ tab)
                                text = text.replace('\n', ' ').replace(
                                    '\r', '').replace('\t', ' ')
                                text = ' '.join(text.split())

                                if not text:
                                    continue

                                # å¾ crops/ ç›®éŒ„è¤‡è£½åˆ°å°æ‡‰çš„ split ç›®éŒ„
                                try:
                                    src_crop = self.crops_dir / crop_filename
                                    if not src_crop.exists():
                                        print(
                                            f"  âš ï¸  Crop not found: {crop_filename}")
                                        continue

                                    dst_crop = split_dir / crop_filename
                                    shutil.copy(src_crop, dst_crop)

                                    # å¯«å…¥ gt.txt (tab åˆ†éš”: filename\ttext)
                                    f.write(f"{crop_filename}\t{text}\n")
                                    total_samples += 1
                                    print(
                                        f"  âœ“ {crop_filename}: {text} (ä¿¡å¿ƒåº¦: {confidence:.2f})")

                                except Exception as e:
                                    print(
                                        f"  âš ï¸  è™•ç† {crop_filename} å¤±æ•—: {e}")
                                    continue

                        else:
                            # æ¨¡å¼ 2: å®Œæ•´åœ–ç‰‡ (è¨“ç·´å®Œæ•´ OCR)
                            try:
                                dst_img = split_dir / image_name
                                shutil.copy(src_img, dst_img)

                                # æº–å‚™æ¨™è¨»æ–‡å­— (å–®è¡Œ,ç§»é™¤æ›è¡Œç¬¦å’Œ tab)
                                label_text = anno.get(
                                    'corrected_text') or anno['full_text']

                                # æ¸…ç†æ–‡å­—
                                label_text = label_text.replace(
                                    '\n', ' ').replace('\r', '').replace('\t', ' ')
                                label_text = ' '.join(label_text.split())

                                if not label_text:
                                    print(f"  âš ï¸  è·³éç©ºæ¨™ç±¤: {image_name}")
                                    continue

                                # å¯«å…¥ gt.txt (tab åˆ†éš”: filename\ttext)
                                f.write(f"{image_name}\t{label_text}\n")
                                total_samples += 1

                                print(f"  âœ“ {image_name}")

                            except Exception as e:
                                print(f"  âš ï¸  è™•ç† {image_name} å¤±æ•—: {e}")
                                continue

                print(f"âœ… Created {split_name} set: {total_samples} samples")
                print(f"   ğŸ“„ gt.txt: {gt_file}")
                print(f"   ğŸ“‚ Images: {split_dir}")

            except (IOError, OSError) as e:
                print(f"âŒ ç„¡æ³•å‰µå»º gt.txt æ–‡ä»¶ {gt_file}: {e}")
                continue
            except Exception as e:
                print(f"âŒ æœªé æœŸçš„éŒ¯èª¤: {type(e).__name__}: {e}")
                continue

        print(f"\n{'='*70}")
        print(f"âœ… Training dataset generated in {self.dataset_dir}")
        print(f"ğŸ“„ Format: gt.txt (tab-separated)")
        print(f"{'='*70}")
        print(f"\nğŸ“– Next steps:")
        print(f"\n1. Convert gt.txt to LMDB format:")
        print(f"   python deep-text-recognition-benchmark/create_lmdb_dataset.py \\")
        print(f"       --inputPath {self.dataset_dir}/train \\")
        print(f"       --gtFile {self.dataset_dir}/train/gt.txt \\")
        print(f"       --outputPath dataset_lmdb/train")
        print(f"   ")
        print(f"   python deep-text-recognition-benchmark/create_lmdb_dataset.py \\")
        print(f"       --inputPath {self.dataset_dir}/valid \\")
        print(f"       --gtFile {self.dataset_dir}/valid/gt.txt \\")
        print(f"       --outputPath dataset_lmdb/valid")
        print(f"\n2. Start training:")
        print(f"   cd deep-text-recognition-benchmark")
        print(f"   python train.py \\")
        print(f"       --train_data ../dataset_lmdb/train \\")
        print(f"       --valid_data ../dataset_lmdb/valid \\")
        print(f"       --Transformation TPS \\")
        print(f"       --FeatureExtraction ResNet \\")
        print(f"       --SequenceModeling BiLSTM \\")
        print(f"       --Prediction Attn")
        print()

    def show_statistics(self):
        """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
        total = len(self.annotations)
        verified = sum(1 for v in self.annotations.values()
                       if v.get('verified', False))
        manual_corrected = sum(
            1 for v in self.annotations.values() if v.get('manual_corrected', False))

        print(f"\n{'='*70}")
        print(f"ğŸ“Š Dataset Statistics")
        print(f"{'='*70}")
        print(f"Total images: {total}")
        print(
            f"Verified: {verified} ({verified/total*100:.1f}%)" if total > 0 else "Verified: 0")
        print(f"Manual corrected: {manual_corrected}")
        print(f"Pending verification: {total - verified}")
        print(f"{'='*70}\n")


def main():
    parser = argparse.ArgumentParser(description='é¦™æ¸¯æ”¶æ“š OCR æ•¸æ“šé›†å‰µå»ºå·¥å…·')
    parser.add_argument('--input', default='input', help='åŸå§‹æ”¶æ“šåœ–ç‰‡è³‡æ–™å¤¾')
    parser.add_argument('--processed', default='processed', help='è™•ç†çµæœè³‡æ–™å¤¾')
    parser.add_argument('--crops', default='crops', help='åˆ‡å‰²å€åŸŸè³‡æ–™å¤¾')
    parser.add_argument('--dataset', default='dataset_gt',
                        help='æœ€çµ‚æ•¸æ“šé›†è³‡æ–™å¤¾(gt.txtæ ¼å¼)')
    parser.add_argument('--mode', choices=['auto', 'generate', 'stats', 'all'],
                        default='auto', help='é‹è¡Œæ¨¡å¼')
    parser.add_argument('--overwrite', action='store_true', help='è¦†è“‹å·²æœ‰çš„æ¨™è¨»')
    parser.add_argument('--auto-verify', action='store_true',
                        help='è‡ªå‹•é©—è­‰æ‰€æœ‰æ¨™è¨»(è·³éæ‰‹å‹•æª¢æŸ¥)')

    args = parser.parse_args()

    # å‰µå»ºæ•¸æ“šé›†å‰µå»ºå™¨
    creator = ReceiptDatasetCreator(
        args.input, args.processed, args.crops, args.dataset, enable_correction=False)

    print("âœ¨ æ¨¡å¼: ä½¿ç”¨åŸåœ–ç›´æ¥é€²è¡Œ OCR")
    print("   - ä¸åšä»»ä½•åœ–åƒé è™•ç†")
    print("   - åªä¿ç•™é«˜ä¿¡å¿ƒåº¦çµæœ (>= 0.5)")
    print("   - åˆ‡å‰²æ–‡å­—å€åŸŸç”¨æ–¼è¨“ç·´\n")

    if args.mode == 'auto':
        print("\nğŸ¤– Mode: Auto-generate annotations")
        creator.auto_generate_annotations(overwrite=args.overwrite)
        creator.show_statistics()
        print("\nğŸ’¡ Next step:")
        print("  Run with --mode generate --auto-verify to create training dataset")

    elif args.mode == 'generate':
        print("\nğŸ“¦ Mode: Generate training dataset")

        # å¦‚æœå•Ÿç”¨è‡ªå‹•é©—è­‰,æ¨™è¨˜æ‰€æœ‰ç‚ºå·²é©—è­‰
        if args.auto_verify:
            print("âš¡ Auto-verify enabled: marking all annotations as verified")
            for anno in creator.annotations.values():
                anno['verified'] = True
            creator.save_annotations()

        creator.show_statistics()
        creator.generate_training_dataset()

    elif args.mode == 'all':
        print("\nğŸš€ Mode: Complete pipeline (auto + generate)")

        # Step 1: è‡ªå‹•æ¨™è¨»
        print("\n" + "="*70)
        print("Step 1/2: Auto-generate annotations")
        print("="*70)
        creator.auto_generate_annotations(overwrite=args.overwrite)

        # Step 2: è‡ªå‹•é©—è­‰ä¸¦ç”Ÿæˆæ•¸æ“šé›†
        print("\nâš¡ Auto-verify: marking all annotations as verified")
        for anno in creator.annotations.values():
            anno['verified'] = True
        creator.save_annotations()

        # Step 3: ç”Ÿæˆæ•¸æ“šé›†
        print("\n" + "="*70)
        print("Step 2/2: Generate training dataset")
        print("="*70)
        creator.generate_training_dataset()

        print("\n" + "="*70)
        print("âœ… Complete pipeline finished!")
        print("="*70)

    elif args.mode == 'stats':
        creator.show_statistics()


if __name__ == '__main__':
    main()
